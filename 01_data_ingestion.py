# Databricks notebook source
# MAGIC %run ./utils/include

# COMMAND ----------

# MAGIC %md
# MAGIC ## Fetching the data & initial model
# MAGIC Now will fetch all the data required for further process, which will include
# MAGIC * anonymized DNS data
# MAGIC * a GeoIP lookup database
# MAGIC * a thread feed
# MAGIC * domain generated by `dnstwist` for our enrichment pipeline
# MAGIC
# MAGIC we will also include:
# MAGIC * top 100k domains on alexa
# MAGIC * a list of dictionary words
# MAGIC * a list of dga domains to train a DGA model

# COMMAND ----------

# MAGIC %sh 
# MAGIC if [ -d /tmp/dns-notebook-datasets ]; then
# MAGIC   cd /tmp/dns-notebook-datasets
# MAGIC   git pull
# MAGIC else
# MAGIC   cd /tmp
# MAGIC   git clone --depth 1 https://github.com/zaferbil/dns-notebook-datasets.git
# MAGIC fi

# COMMAND ----------

# Copy the downloaded data into the FileStore for this workspace
print(f'Copying datasets and model to the DBFS: {get_default_path()}')
dbutils.fs.cp("file:///tmp/dns-notebook-datasets/data", f"dbfs:{get_default_path()}/datasets/",True)
dbutils.fs.cp("file:///tmp/dns-notebook-datasets/model", f"dbfs:{get_default_path()}/model/",True)

# COMMAND ----------

# MAGIC %md
# MAGIC ## Loading pDNS data
# MAGIC Defining the schema for pDNS.
# MAGIC
# MAGIC You can either use the python style syntax or the SQL DDL syntax to define your schema.

# COMMAND ----------

# DBTITLE 1,Defining schema in Python
from pyspark.sql.types import StructType, StructField, StringType, LongType, StringType, ArrayType
# Way: 1
pdns_schema = (
    StructType([
        StructField("rrname", StringType(), True),
        StructField("rrtype", StringType(), True),
        StructField("time_first", LongType(), True),
        StructField("time_last", LongType(), True),
        StructField("count", LongType(), True),
        StructField("bailwick", StringType(), True),
        StructField("rdata", ArrayType(StringType(), True), True),
    ])
)

# Way: 2
pdns_schema = (
    StructType()
    .add("rrname", StringType(), True)
    .add("rrtype", StringType(), True)
    .add("time_first", LongType(), True)
    .add("time_last", LongType(), True)
    .add("count", LongType(), True)
    .add("bailwick", StringType(), True)
    .add("", ArrayType(StringType(), True), True)
)

# COMMAND ----------

# DBTITLE 1,SQL DDL syntax
pdns_schema = """
  rrname     string,
  rrtype     string,
  time_first long,
  time_last  long,
  count      long,
  bailiwick  string,
  rdata      array<string>
"""

# COMMAND ----------

# In this segment, we are specifying where the data is and what type of data it is.
# You can see the json format, the path and the AWS region
df = spark.read.format("json").schema(pdns_schema).load(f"{get_default_path()}/datasets/dns_events.json")

# COMMAND ----------

# the rdata field has an array element. This isn't very userfull if you want to parse it, or search in it.
# so we create a new field called rdatastr. You can see the difference in the two fields in the smaple output below.
from pyspark.sql.functions import col, concat_ws
df_enhanced = df.withColumn("rdatastr", concat_ws(",", col("rdata")))
display(df_enhanced)

# COMMAND ----------

# Here we specify the format of the data to be written, and the destination path
# This is still just setup - Data has not been posted to the Bronze table yet. 
df_enhanced.write.format("delta").mode("overwrite").option("mergeSchema", "true").saveAsTable("bronze_dns")

# COMMAND ----------

# MAGIC %md
# MAGIC ## URLHaus threat feed setup
# MAGIC We will be using URLHaus threat feeds with our pDNS data. This section shows you how to ingest the URLHaus feed.
# MAGIC
# MAGIC For this setup, we need to do two things:
# MAGIC
# MAGIC * Define functions for field extractions so we can extract the `registered_domain_extract`, `domain_extract` and `suffix_extract` fields from the URLHaus feeds. This is done via user defined functions (UDF) that are declared in the `./Shared_Include` notebook.
# MAGIC * Create an enriched schema and save it to a silver table.

# COMMAND ----------

# We spcify the source location of the URLHaus feed, the csv feed, the csv format, and declare the csv has field labels in a header
threat_feeds_location = f"{get_default_path()}/datasets/ThreatDataFeed.txt"
threat_feeds_raw = spark.read.csv(threat_feeds_location, header=True)
# display a sample so we can check to see it makes sense
display(threat_feeds_raw)

# COMMAND ----------

# We create a new enriched view by extracting the domain name from the URL using the domain_extractor user defined function from the previous section.
threat_feeds_raw.createOrReplaceTempView("threat_feeds_raw")
threat_feeds_enriched_df = spark.sql("""
  select *, domain_extract(url) as domain
  from threat_feeds_raw
  """).filter("char_length(domain) >= 2")
# the sample display shows the new field "domain"
display(threat_feeds_enriched_df)

# COMMAND ----------

# We save our new, enriched schema 
(threat_feeds_enriched_df.write
  .format("delta")
  .mode('overwrite')
  .option("mergeSchema", True)
  .saveAsTable("silver_threat_feeds")
)

# COMMAND ----------

# MAGIC %md
# MAGIC ## DNS Twist Setup for detecting lookalike domains
# MAGIC We will use [dnstwist](https://github.com/elceef/dnstwist) to monitor lookalike domains that adversaries can use to attack you. Using dnstwist you can detect [typosquatters](https://capec.mitre.org/data/definitions/630.html), phishing attacks, fraud, and brand impersonation. Before using the remainder of section 1.b of this notebook, you will have to use [dnstwist instructions](https://github.com/elceef/dnstwist) (outside of this notebook) to create a domains_dnstwists.csv. In our example (below) we generated variations for `google.com` using `dnstwist`. You can automate this for your own organization or for any organization of interest.
# MAGIC
# MAGIC We formatted domains_dnstwists.csv with a header: `PERMUTATIONTYPE`,`domain`,`meta`
# MAGIC
# MAGIC Once you have created `domain_dnstwists.csv`, you can continue:
# MAGIC
# MAGIC * load the dnstwisted domains
# MAGIC * enrich the table with domain names (without TLDs)
# MAGIC * load the `dnstwist`-enriched results into a silver table
# MAGIC We will use these tables later to productionize typosquatting detection.

# COMMAND ----------

# NOTE: domain_dnstwists.csv needs to be created outside of this notebook, using instructions from dnstwist. 
# Load the domain_dnstwists.csv into a dataframe, brand_domains_monitored_raw. Note the csv and header, true options.
brand_domains_monitored_raw_df = spark.read.csv(f"{get_default_path()}/datasets/domains_dnstwists.csv", header=True) 

# COMMAND ----------

display(brand_domains_monitored_raw_df)

# COMMAND ----------

# Load the csv brand_domains_monitored_raw into a local table called, brand_domains_monitored_raw
brand_domains_monitored_raw_df.createOrReplaceTempView("brand_domains_monitored_raw")

# COMMAND ----------

# Extract the domain names using the UDF we created at Cmd 9 of this notebook.
# Create a new table with the dnstwist extracted domains. New column dnstwisted_domain
# The hardcoded ">=2" is there to accommodate for potential empty domain fields
brand_domains_monitored_enriched_df = spark.sql("""
  select *, domain_extract(domain) as dnstwisted_domain
  from brand_domains_monitored_raw
  """).filter("char_length(dnstwisted_domain) >= 2")
display(brand_domains_monitored_enriched_df)

# COMMAND ----------

# Define a silver Delta table
(brand_domains_monitored_enriched_df.write
  .format("delta")
  .mode('overwrite')
  .option("mergeSchema", False)
  .saveAsTable("silver_twisted_domain_brand")
)

# COMMAND ----------

# MAGIC %sql
# MAGIC /* Query the silver Delta table */
# MAGIC select *  from silver_twisted_domain_brand
